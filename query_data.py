# from openai import OpenAI
# from langchain_chroma import Chroma
# from langchain.prompts import ChatPromptTemplate

# from get_embedding_function import get_embedding_function

# positive_keywords = ["priorit√©", "signalisation", "panneau", "intersection", "circulation", "voie", "stop", "feu tricolore"]
# negative_keywords = ["immobilisation", "remorquage", "accident", "d√©pannage", "bris", "enl√®vement", "panne", "fourri√®re", "infractions graves"]

# def filter_documents(docs):
#     filtered_docs = []
#     for doc in docs:
#         content = doc.page_content.lower()
#         if any(pk in content for pk in positive_keywords) and not any(nk in content for nk in negative_keywords):
#             filtered_docs.append(doc)
#     return filtered_docs


# CHROMA_PATH = "chroma"

# PROMPT_TEMPLATE = """
# Vous √™tes un expert du Code de la Route et un observateur de sc√®nes visuelles.

# Votre d√©cision (GO ou STOP) doit √™tre bas√©e **prioritairement** sur l'analyse directe de la sc√®ne. 
# La recherche documentaire ci-dessous (issu du Code de la Route) doit uniquement servir √† v√©rifier ou renforcer votre d√©cision initiale, **sans la contredire** si la sc√®ne est claire et sans danger visible.

# {context}

# ---

# Answer the question based on the above context: {question}
# """


# # ‚úÖ Initialize OpenAI Client globally
# client = OpenAI(
#     api_key="FXMLo8lTdaSqsmDDRuJ3mT55xcJCryQy",
#     base_url="https://llm.intellisphere.fr:9081/v1",
#     timeout=60
# )

# # ‚úÖ Hardcode the model name
# model_name = "Qwen/Qwen2.5-VL-7B-Instruct"

# def query_rag(query_text: str):
#     # Prepare the DB
#     embedding_function = get_embedding_function()
#     db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

#     # Search the DB
#     results = db.similarity_search_with_score(query_text, k=5)

#     context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
#     prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
#     prompt = prompt_template.format(context=context_text, question=query_text)

#     # üõ†Ô∏è Use OpenAI client directly, not OllamaLLM
#     response = client.chat.completions.create(
#         model=model_name,
#         messages=[
#             {"role": "system", "content": "Tu es un assistant expert en droit du Code de la Route."},
#             {"role": "user", "content": prompt}
#         ],
#         temperature=0.7,
#         top_p=0.9
#     )

#     response_text = response.choices[0].message.content

#     sources = [doc.metadata.get("id", None) for doc, _score in results]
#     formatted_response = f"Response: {response_text}\nSources: {sources}"
#     print(formatted_response)
#     return response_text



# if __name__ == "__main__":
#     main()

# query_data.py

from openai import OpenAI
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate

from get_embedding_function import get_embedding_function

CHROMA_PATH = "chroma"

# PROMPT_TEMPLATE = """
# Vous √™tes un expert du Code de la Route et un observateur de sc√®nes visuelles.

# Voici l'analyse pr√©liminaire d'un mod√®le de vision sur une sc√®ne :
# "{vision_response}"

# Bas√© sur cette analyse et en utilisant uniquement les extraits documentaires suivants issus du Code de la Route :

# {context}

# ---

# D√©cidez si le v√©hicule peut partir en r√©pondant uniquement par 'GO' ou 'STOP'. 
# Justifiez bri√®vement en citant une r√®gle applicable, en confirmant ou infirmant l'analyse pr√©liminaire si n√©cessaire.
# """

PROMPT_TEMPLATE = """
Vous √™tes un expert du Code de la Route et un observateur de sc√®nes visuelles.

Voici la description d'une sc√®ne routi√®re observ√©e depuis la perspective du conducteur :
"{vision_response}"

En vous basant uniquement sur cette description visuelle et sur les extraits documentaires suivants issus du Code de la Route :

{context}

---

Votre t√¢che :
- D√©cidez si le v√©hicule peut partir (**GO**) ou doit rester arr√™t√© (**STOP**) ou "D√©marrer lentement".
- Justifiez votre d√©cision en citant une r√®gle applicable du Code de la Route.

Format de r√©ponse :
- Commencez par **GO** ou **STOP** ou "D√©marrer lentement"(en majuscules).
- Ajoutez une justification tr√®s courte (1 √† 2 phrases maximum).

Important :
- N'inventez pas d'√©l√©ments absents de la description.
- Ne donnez jamais une r√©ponse sans justification l√©gale ou observation logique bas√©e sur la sc√®ne.
"""



client = OpenAI(
    api_key="FXMLo8lTdaSqsmDDRuJ3mT55xcJCryQy",
    base_url="https://llm.intellisphere.fr:9081/v1",
    timeout=60
)

model_name = "Qwen/Qwen2.5-VL-7B-Instruct"

def query_rag(vision_response: str):
    # üîé Pr√©paration de la base de donn√©es
    embedding_function = get_embedding_function()
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # üìö Recherche dans Chroma bas√©e sur la r√©ponse vision
    results = db.similarity_search_with_score(vision_response, k=5)

    # üß© Construction du contexte
    context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])

    # üìù Cr√©ation du prompt
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context_text, vision_response=vision_response)

    # üöÄ Envoi √† LLM pour finaliser la r√©ponse
    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "Tu es un assistant expert en droit du Code de la Route."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        top_p=0.9
    )

    # üìú Extraction de la r√©ponse
    response_text = response.choices[0].message.content

    # üìë Extraction des sources
    sources = [doc.metadata.get("id", None) for doc, _score in results if doc.metadata.get("id", None)]

    # üéØ Formatage final
    formatted_response = f"{response_text}\n\nüîó Sources utilis√©es : {sources if sources else 'Aucune source disponible.'}"
    
    print(formatted_response)
    return formatted_response

if __name__ == "__main__":
    raise Exception("Ce script n'est pas cens√© √™tre ex√©cut√© directement.")
